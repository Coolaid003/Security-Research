# Vulnerability detail about CVE-2024-41010

## Trigger Vulnerability

The vulnerability occurs because `tcx_entry_fetch_or_create()`, which allocates `tcx_entry` objects, does not manage these objects using a reference count-based system. As a result, functions such as `clsact_init()`, `ingress_init()`, and `tcx_prog_attach()` are affected.

The `tcx_entry_fetch_or_create()` is called in the following call stack when creating ingress qdisc/clsact qdisc:

```c
rtnetlink_rcv_msg()
  => tc_modify_qdisc()
    => qdisc_create()
      => ingress_init()
        => tcx_entry_fetch_or_create()
    => qdisc_graft()
```

This function calls `tcx_entry_create()` to allocate and link a new `tcx_entry` if none is registered with the network device [1]. If a `tcx_entry` is already registered, it retrieves it by calling `tcx_entry_fetch()` [2].

```jsx
static inline struct bpf_mprog_entry *
tcx_entry_fetch(struct net_device *dev, bool ingress)
{
	ASSERT_RTNL();
	if (ingress)
		return rcu_dereference_rtnl(dev->tcx_ingress);    // <==[3]
	else
		return rcu_dereference_rtnl(dev->tcx_egress);
}

static inline struct bpf_mprog_entry *
tcx_entry_fetch_or_create(struct net_device *dev, bool ingress, bool *created)
{
	struct bpf_mprog_entry *entry = tcx_entry_fetch(dev, ingress);  // <==[2]

	*created = false;
	if (!entry) {
		entry = tcx_entry_create();    // <==[1]
		if (!entry)
			return NULL;
		*created = true;
	}
	return entry;
}

static int ingress_init(struct Qdisc *sch, struct nlattr *opt,
			struct netlink_ext_ack *extack)
{
	struct ingress_sched_data *q = qdisc_priv(sch);
	struct net_device *dev = qdisc_dev(sch);
	struct bpf_mprog_entry *entry;
	bool created;
	int err;

	if (sch->parent != TC_H_INGRESS)
		return -EOPNOTSUPP;

	net_inc_ingress_queue();

	entry = tcx_entry_fetch_or_create(dev, true, &created);
	if (!entry)
		return -ENOMEM;
	tcx_miniq_set_active(entry, true);
	mini_qdisc_pair_init(&q->miniqp, sch, &tcx_entry(entry)->miniq);    // <==[4]
	if (created)
		tcx_entry_update(dev, entry, true);

	q->block_info.binder_type = FLOW_BLOCK_BINDER_TYPE_CLSACT_INGRESS;
	q->block_info.chain_head_change = clsact_chain_head_change;
	q->block_info.chain_head_change_priv = &q->miniqp;

	err = tcf_block_get_ext(&q->block, sch, &q->block_info, extack);
	if (err)
		return err;

	mini_qdisc_pair_block_init(&q->miniqp, q->block);

	return 0;
}
```

The issue arises because there is no logic to increment the refcount, regardless of whether a `tcx_entry` is newly allocated or retrieved. This oversight can lead to problems down the line [3].

The `&tcx_entry->miniq` of this `tcx_entry` is stored in `miniqp->p_miniq` within `mini_qdisc_pair_init()` [4] and is subsequently used as a double pointer [5].

```c
void mini_qdisc_pair_init(struct mini_Qdisc_pair *miniqp, struct Qdisc *qdisc,
                          struct mini_Qdisc __rcu **p_miniq)
{
        miniqp->miniq1.cpu_bstats = qdisc->cpu_bstats;
        miniqp->miniq1.cpu_qstats = qdisc->cpu_qstats;
        miniqp->miniq2.cpu_bstats = qdisc->cpu_bstats;
        miniqp->miniq2.cpu_qstats = qdisc->cpu_qstats;
        miniqp->miniq1.rcu_state = get_state_synchronize_rcu();
        miniqp->miniq2.rcu_state = miniqp->miniq1.rcu_state;
        miniqp->p_miniq = p_miniq;    // <==[5]
}
EXPORT_SYMBOL(mini_qdisc_pair_init);
```

Next, `chain0` is linked to the `tc block` in the following call stack. Here, the `tc block` is pre-created with index 1 when the `ingress qdisc` is first allocated and references the `ingress qdisc`:

```c
rtnetlink_rcv_msg()
  => tc_ctl_chain()
```

Next, when a `clsact qdisc` is allocated to a network device that already has an `ingress qdisc`, the existing `ingress qdisc` is released and replaced by the `clsact qdisc` in the following call stack. During this process, the `tcx_entry` is also released:

```c
rtnetlink_rcv_msg()
  => tc_modify_qdisc()
    => qdisc_create()
      => clsact_init()
        => tcf_block_get_ext()
          => tcf_chain0_head_change_cb_add()
    => qdisc_graft()
      => qdisc_destroy()
        => __qdisc_destroy()
          => ingress_destroy()
            => tcx_entry_free()
              => kfree_rcu()
```

In the process of allocating this clsact qdisc, the `tcf_chain0_head_change_cb_add()` function is called. Since `chain0` was previously assigned to the `tc block` with index 1, connecting this `clsact qdisc` to the tc block with index 1 will also link an item to `&block->chain0.filter_chain_list` [6].

```c
static int
tcf_chain0_head_change_cb_add(struct tcf_block *block,
                              struct tcf_block_ext_info *ei,
                              struct netlink_ext_ack *extack)
{
        struct tcf_filter_chain_list_item *item;
        struct tcf_chain *chain0;

        item = kmalloc(sizeof(*item), GFP_KERNEL);
        if (!item) {
                NL_SET_ERR_MSG(extack, "Memory allocation for head change callback item failed");
                return -ENOMEM;
        }
        item->chain_head_change = ei->chain_head_change;
        item->chain_head_change_priv = ei->chain_head_change_priv;

        mutex_lock(&block->lock);
        chain0 = block->chain0.chain;
        if (chain0)
                tcf_chain_hold(chain0);
        else
                list_add(&item->list, &block->chain0.filter_chain_list);
        mutex_unlock(&block->lock);

        if (chain0) {
                struct tcf_proto *tp_head;

                mutex_lock(&chain0->filter_chain_lock);

                tp_head = tcf_chain_dereference(chain0->filter_chain, chain0);
                if (tp_head)
                        tcf_chain_head_change_item(item, tp_head);

                mutex_lock(&block->lock);
                list_add(&item->list, &block->chain0.filter_chain_list);    // <==[6]
                mutex_unlock(&block->lock);

                mutex_unlock(&chain0->filter_chain_lock);
                tcf_chain_put(chain0);
        }

        return 0;
}
```

Finally, when the namespace to which the current network device belongs is closed, the `cleanup_net()` worker is called, resulting in a Use-After-Free condition.

```c
cleanup_net()
  => ops_exit_list()
    => default_device_exit_batch()
      => unregister_netdevice_many()
        => unregister_netdevice_many_notify()
          => dev_shutdown()
            => qdisc_put()
              => clsact_destroy()
                => tcf_block_put_ext()
                  => tcf_chain0_head_change_cb_del()
                    => tcf_chain_head_change_item()
                      => clsact_chain_head_change()
                        => mini_qdisc_pair_swap()
```

The function where the Use-After-Free ultimately occurs is `mini_qdisc_pair_swap()`. This function retrieves `*miniqp->p_miniq` [7] and writes a value to `->rcu_state` [8]. Since `miniqp->p_miniq` is pointing to the already freed `&tcx_entry->miniq` [5], a UAF condition occurs.
